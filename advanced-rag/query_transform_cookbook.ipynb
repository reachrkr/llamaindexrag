{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "331f5d8c-3041-4d69-a957-93a79b335562",
      "metadata": {
        "id": "331f5d8c-3041-4d69-a957-93a79b335562"
      },
      "source": [
        "# Query Transformation Techniques - llamaindex\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/query_transform_cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "A user query can be transformed and decomposed in many ways before being executed as part of a RAG query engine, agent, or any other pipeline.\n",
        "\n",
        "In this guide we show you different ways to transform, decompose queries, and find the set of relevant tools. Each technique might be applicable for different use cases!\n",
        "\n",
        "Here are the different query transformations:\n",
        "\n",
        "\n",
        "1. **Query-Rewriting**: Keep the tools, but rewrite the query in a variety of different ways to execute against the same tools.\n",
        "2. **Sub-Questions**: Decompose queries into multiple sub-questions over different tools (identified by their metadata).\n",
        "3. **ReAct Agent Tool Picking**: Given the initial query, identify 1) the tool to pick, and 2) the query to execute on the tool.\n",
        "4. **Multi-Step Query Decomposition** : A multi-step query engine that's able to decompose a complex query into sequential subquestions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c100403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c100403",
        "outputId": "fa68bace-e774-4dd8-be03-29d6311ab99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-question-gen-openai in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-question-gen-openai) (0.10.13)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-question-gen-openai) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-question-gen-openai) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.9.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-program-openai<0.2.0,>=0.1.1->llama-index-question-gen-openai) (0.1.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-question-gen-openai) (1.16.0)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai) (0.10.13)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-openai) (1.16.0)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.13.post1)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.5)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.5)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.13 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.13)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-vector-stores-chroma<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.25)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.0.2)\n",
            "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.13->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.14.1)\n",
            "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.4.23)\n",
            "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.17.1)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.15.2)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.13->llama-index) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.13->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.13->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.7.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /usr/local/lib/python3.10/dist-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.13->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.13->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.13->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.13->llama-index) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.13->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.13->llama-index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.110.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.27.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.4.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.4.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.60.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (29.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.9.15)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.13->llama-index) (23.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.13->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.13->llama-index) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.13->llama-index) (1.16.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.20.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.36.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.23.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (67.7.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (12.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-question-gen-openai\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30451d5d-91a8-4148-bdf8-a4b4fd8ab0fc",
      "metadata": {
        "id": "30451d5d-91a8-4148-bdf8-a4b4fd8ab0fc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# define prompt viewing function\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        print(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0023e21-1bd2-46cb-b829-c697d1132862",
      "metadata": {
        "id": "c0023e21-1bd2-46cb-b829-c697d1132862"
      },
      "source": [
        "## Routing\n",
        "\n",
        "In this example, we show how a query can be used to select the set of relevant tool choices.\n",
        "\n",
        "We use our `selector` abstraction to pick the relevant tool(s) - it can be a single tool, or a multiple tool depending on the abstraction.\n",
        "\n",
        "We have four selectors: combination of (LLM or function calling) x (single selection or multi-selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ec907783-a091-4ae8-b8a5-d81ae3ede92b",
      "metadata": {
        "id": "ec907783-a091-4ae8-b8a5-d81ae3ede92b"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
        "from llama_index.core.selectors import (\n",
        "    PydanticMultiSelector,\n",
        "    PydanticSingleSelector,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# platform.openai.com\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
        "    \"Enter OpenAI API Key: \"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6XeDhWyyBfU",
        "outputId": "ccd16629-df5d-41bd-c84c-02be3995a25a"
      },
      "id": "e6XeDhWyyBfU",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API Key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f5000dc-17db-4a1b-8859-b6651a2a9465",
      "metadata": {
        "id": "6f5000dc-17db-4a1b-8859-b6651a2a9465"
      },
      "outputs": [],
      "source": [
        "\n",
        "selector = LLMMultiSelector.from_defaults()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(selector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd4h5Tzx1KlJ",
        "outputId": "9b960820-babb-4054-ebc9-ba7a890bd4b2"
      },
      "id": "sd4h5Tzx1KlJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<llama_index.core.selectors.llm_selectors.LLMMultiSelector object at 0x7f466ca08be0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21954a93-e2f2-460b-8073-f6a8a88ad1d2",
      "metadata": {
        "id": "21954a93-e2f2-460b-8073-f6a8a88ad1d2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import ToolMetadata\n",
        "\n",
        "tool_choices = [\n",
        "    ToolMetadata(\n",
        "        name=\"Apples_chinese\",\n",
        "        description=(\"This tool contains a Chinese apple variants\"),\n",
        "    ),\n",
        "    ToolMetadata(\n",
        "        name=\"Apples_australian\",\n",
        "        description=(\"This tool contains the Australian apple variants\"),\n",
        "    ),\n",
        "    ToolMetadata(\n",
        "        name=\"Apples_washington\",\n",
        "        description=(\"This tool contains the washinghton breed of apples\"),\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a559269-5129-4474-aade-ee6039ff4066",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "2a559269-5129-4474-aade-ee6039ff4066",
        "outputId": "d1477ea1-94a3-4444-f387-bde21c73d765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Prompt Key**: prompt<br>**Text:** <br>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n",
            "---------------------\n",
            "{context_list}\n",
            "---------------------\n",
            "Using only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\n",
            "\n",
            "\n",
            "The output should be ONLY JSON formatted as a JSON instance.\n",
            "\n",
            "Here is an example:\n",
            "[\n",
            "    {{\n",
            "        choice: 1,\n",
            "        reason: \"<insert reason for choice>\"\n",
            "    }},\n",
            "    ...\n",
            "]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_prompt_dict(selector.get_prompts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1E5e_d5-2XgC"
      },
      "id": "1E5e_d5-2XgC",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cec2fba5-8e5e-4125-9061-56ae9c707329",
      "metadata": {
        "id": "cec2fba5-8e5e-4125-9061-56ae9c707329"
      },
      "outputs": [],
      "source": [
        "selector_result = selector.select(\n",
        "    tool_choices, query=\"Tell me more about Apples\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EtXup1g12W5"
      },
      "id": "4EtXup1g12W5",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1cb4b53a-0861-4300-b593-ad63edb53e96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cb4b53a-0861-4300-b593-ad63edb53e96",
        "outputId": "5fe55745-0c37-4cf5-bf16-8c7b7364b2ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SingleSelection(index=2, reason='Washington breed of apples is a popular and well-known variety')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "selector_result.selections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc787575-e60c-4c06-8b39-b33230438e00",
      "metadata": {
        "id": "cc787575-e60c-4c06-8b39-b33230438e00"
      },
      "source": [
        "## Query Rewriting\n",
        "\n",
        "In this section, we show you how to rewrite queries into multiple queries. You can then execute all these queries against a retriever.\n",
        "\n",
        "This is a key step in advanced retrieval techniques. By doing query rewriting, you can generate multiple queries for [ensemble retrieval] and [fusion], leading to higher-quality retrieved results.\n",
        "\n",
        "Unlike the sub-question generator, this is just a prompt call, and exists independently of tools."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38c5be2-fd99-4987-8412-2eda597da412",
      "metadata": {
        "id": "b38c5be2-fd99-4987-8412-2eda597da412"
      },
      "source": [
        "### Query Rewriting (Custom)\n",
        "\n",
        "Here we show you how to use a prompt to generate multiple queries, using our LLM and prompt abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d969dfcb-0a2b-4178-87ac-1d0444981f49",
      "metadata": {
        "id": "d969dfcb-0a2b-4178-87ac-1d0444981f49"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "query_gen_str = \"\"\"\\\n",
        "You are a helpful assistant that generates multiple search queries based on a \\\n",
        "single input query. Generate {num_queries} search queries, one on each line, \\\n",
        "related to the following input query:\n",
        "Query: {query}\n",
        "Queries:\n",
        "\"\"\"\n",
        "query_gen_prompt = PromptTemplate(query_gen_str)\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "def generate_queries(query: str, llm, num_queries: int = 4):\n",
        "    response = llm.predict(\n",
        "        query_gen_prompt, num_queries=num_queries, query=query\n",
        "    )\n",
        "    # assume LLM proper put each query on a newline\n",
        "    queries = response.split(\"\\n\")\n",
        "    queries_str = \"\\n\".join(queries)\n",
        "    print(f\"Generated queries:\\n{queries_str}\")\n",
        "    return queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4a4f1ed7-cbd8-4177-9c29-a6ecb2e8a530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a4f1ed7-cbd8-4177-9c29-a6ecb2e8a530",
        "outputId": "b947e769-1579-4d49-c13b-02509556092d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated queries:\n",
            "1. Latest news on Citrix incident\n",
            "2. Citrix security breach details\n",
            "3. Impact of Citrix incident on customers\n",
            "4. Citrix response to recent events\n"
          ]
        }
      ],
      "source": [
        "queries = generate_queries(\"What happened at Citrix\", llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f416f6ba-efc8-4e30-84ea-e20466168758",
      "metadata": {
        "id": "f416f6ba-efc8-4e30-84ea-e20466168758"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "535b3f78-1036-4878-b0fe-44f5330fdb1e",
      "metadata": {
        "id": "535b3f78-1036-4878-b0fe-44f5330fdb1e"
      },
      "source": [
        "### Query Rewriting (using HyDE Query Transform)\n",
        "\n",
        "In this section we show you how to do query transformations using our Hyde QueryTransform class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3ac0639a-65ae-43cf-a68a-c9705cf7cb12",
      "metadata": {
        "id": "3ac0639a-65ae-43cf-a68a-c9705cf7cb12"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine import TransformQueryEngine\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAvD9xJB85ze",
        "outputId": "df7197f6-3e14-43f0-b692-9aff95e7943c"
      },
      "id": "pAvD9xJB85ze",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 08:45:05--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: data/paul_graham/paul_graham_essay.txt\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-27 08:45:05 (4.45 MB/s) - data/paul_graham/paul_graham_essay.txt saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "480d58fc-b5c6-41d0-b497-d147a2d1e471",
      "metadata": {
        "id": "480d58fc-b5c6-41d0-b497-d147a2d1e471"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n"
      ],
      "metadata": {
        "id": "C_3I5I4fBF6o"
      },
      "id": "C_3I5I4fBF6o",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we query without transformation: The same query string is used for embedding lookup and also summarization."
      ],
      "metadata": {
        "id": "OG-l06bgBLrS"
      },
      "id": "OG-l06bgBLrS"
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = \"what did paul graham do after going to RISD\""
      ],
      "metadata": {
        "id": "Lf3UD56-BMRn"
      },
      "id": "Lf3UD56-BMRn",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(query_str)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bn9x5CamBT9a",
        "outputId": "5eace69f-b75d-4025-e55c-557fe96fa7fd"
      },
      "id": "Bn9x5CamBT9a",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Paul Graham started painting still lives in his bedroom at night while he was a student at the Accademia di Belli Arti in Florence.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup."
      ],
      "metadata": {
        "id": "PD9udZsfBmwt"
      },
      "id": "PD9udZsfBmwt"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "61994183-0b39-4d9a-ae74-b78f68b8dcfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "61994183-0b39-4d9a-ae74-b78f68b8dcfc",
        "outputId": "9a7238f8-5927-44f8-fa4a-ec316cc7f905"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>After going to RISD, Paul Graham resumed his old patterns and started looking for an apartment to buy. He also had an idea to build a web app for making web apps, which eventually led him to start a new company called Aspra.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
        "response = hyde_query_engine.query(query_str)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_bundle = hyde(query_str)\n",
        "hyde_doc = query_bundle.embedding_strs[0]"
      ],
      "metadata": {
        "id": "F1z7natgDbce"
      },
      "id": "F1z7natgDbce",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ktNmMGeqDblc",
        "outputId": "11999a96-b7d7-46b8-c19b-6de41c62e926"
      },
      "id": "ktNmMGeqDblc",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"After attending the Rhode Island School of Design (RISD), Paul Graham went on to co-found Viaweb, one of the first online stores that allowed users to build their own e-commerce websites. Viaweb was later acquired by Yahoo for $49 million, marking Graham's first major success in the tech industry. Following this, Graham went on to co-found Y Combinator, a startup accelerator that has helped launch successful companies such as Dropbox, Airbnb, and Reddit. Graham is also known for his influential essays on startups and entrepreneurship, as well as his work as an angel investor in various tech companies. Overall, Paul Graham's career trajectory after RISD has been marked by innovation, success, and a significant impact on the tech industry.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ee4457-e8dc-4527-b305-9b09fb89c1f7",
      "metadata": {
        "id": "f4ee4457-e8dc-4527-b305-9b09fb89c1f7"
      },
      "source": [
        "## Sub-Questions\n",
        "\n",
        "Given a set of tools and a user query, decide both the 1) set of sub-questions to generate, and 2) the tools that each sub-question should run over.\n",
        "\n",
        "We run through an example using the `OpenAIQuestionGenerator`, which depends on function calling, and also the `LLMQuestionGenerator`, which depends on prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "423c9cc6-20eb-4ae3-bca0-c01e606f865a",
      "metadata": {
        "id": "423c9cc6-20eb-4ae3-bca0-c01e606f865a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.question_gen import LLMQuestionGenerator\n",
        "from llama_index.question_gen.openai import OpenAIQuestionGenerator\n",
        "from llama_index.llms.openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "59f47121-fc91-45cd-a071-c3c6208726b7",
      "metadata": {
        "id": "59f47121-fc91-45cd-a071-c3c6208726b7"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI()\n",
        "question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "effdbaca-4087-4199-bc85-ceea00d6edf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "effdbaca-4087-4199-bc85-ceea00d6edf4",
        "outputId": "6866cff7-8058-479a-fe98-0b402318cc09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Prompt Key**: question_gen_prompt<br>**Text:** <br>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a world class state of the art agent.\n",
            "\n",
            "You have access to multiple tools, each representing a different data source or API.\n",
            "Each of the tools has a name and a description, formatted as a JSON dictionary.\n",
            "The keys of the dictionary are the names of the tools and the values are the descriptions.\n",
            "Your purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n",
            "\n",
            "These are the guidelines you consider when completing your task:\n",
            "* Be as specific as possible\n",
            "* The sub questions should be relevant to the user question\n",
            "* The sub questions should be answerable by the tools provided\n",
            "* You can generate multiple sub questions for each tool\n",
            "* Tools must be specified by their name, not their description\n",
            "* You don't need to use a tool if you don't think it's relevant\n",
            "\n",
            "Output the list of sub questions by calling the SubQuestionList function.\n",
            "\n",
            "## Tools\n",
            "```json\n",
            "{tools_str}\n",
            "```\n",
            "\n",
            "## User Question\n",
            "{query_str}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_prompt_dict(question_gen.get_prompts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "586cf84d-276d-40f3-bb5e-2f5f774b84a1",
      "metadata": {
        "id": "586cf84d-276d-40f3-bb5e-2f5f774b84a1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import ToolMetadata\n",
        "\n",
        "tool_choices = [\n",
        "    ToolMetadata(\n",
        "        name=\"uber_2021_10k\",\n",
        "        description=(\n",
        "            \"Provides information about Uber financials for year 2021\"\n",
        "        ),\n",
        "    ),\n",
        "    ToolMetadata(\n",
        "        name=\"lyft_2021_10k\",\n",
        "        description=(\n",
        "            \"Provides information about Lyft financials for year 2021\"\n",
        "        ),\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3bf94cf9-a869-420f-a6c2-3f2a0b11365e",
      "metadata": {
        "id": "3bf94cf9-a869-420f-a6c2-3f2a0b11365e"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import QueryBundle\n",
        "\n",
        "query_str = \"Compare and contrast Uber and Lyft\"\n",
        "choices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11696baf-4767-4b16-bf41-ea4fe9a3722e",
      "metadata": {
        "id": "11696baf-4767-4b16-bf41-ea4fe9a3722e"
      },
      "source": [
        "The outputs are `SubQuestion` Pydantic objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3cc4baa8-db8e-4357-b3fe-d9975f8c7fa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cc4baa8-db8e-4357-b3fe-d9975f8c7fa1",
        "outputId": "fad9c289-599d-4fa7-93c4-6759e11c695f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuestion(sub_question='What were the total revenues for Uber in 2021?', tool_name='uber_2021_10k'),\n",
              " SubQuestion(sub_question='What were the total revenues for Lyft in 2021?', tool_name='lyft_2021_10k'),\n",
              " SubQuestion(sub_question='What were the net profits for Uber in 2021?', tool_name='uber_2021_10k'),\n",
              " SubQuestion(sub_question='What were the net profits for Lyft in 2021?', tool_name='lyft_2021_10k')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "choices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c9cdc5-e224-4fe8-87c1-06e05d9b5aef",
      "metadata": {
        "id": "81c9cdc5-e224-4fe8-87c1-06e05d9b5aef"
      },
      "source": [
        "## Query Transformation with ReAct Prompt\n",
        "\n",
        "ReAct is a popular framework for agents, and here we show how the core ReAct prompt can be used to transform queries.\n",
        "\n",
        "We use the `ReActChatFormatter` to get the set of input messages for the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cea5bba4-121f-440c-9407-3416f05028fb",
      "metadata": {
        "id": "cea5bba4-121f-440c-9407-3416f05028fb"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import ReActChatFormatter\n",
        "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.llms import ChatMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ce0575c5-75aa-4f8d-bd15-2383769d7401",
      "metadata": {
        "id": "ce0575c5-75aa-4f8d-bd15-2383769d7401"
      },
      "outputs": [],
      "source": [
        "def execute_sql(sql: str) -> str:\n",
        "    \"\"\"Given a SQL input string, execute it.\"\"\"\n",
        "    # NOTE: This is a mock function\n",
        "    return f\"Executed {sql}\"\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "tool1 = FunctionTool.from_defaults(fn=execute_sql)\n",
        "tool2 = FunctionTool.from_defaults(fn=add)\n",
        "tools = [tool1, tool2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d04fb7a-261f-4220-8b3f-821886c0a0c9",
      "metadata": {
        "id": "6d04fb7a-261f-4220-8b3f-821886c0a0c9"
      },
      "source": [
        "Here we get the input prompt messages to pass to the LLM. Take a look!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3b5f2fa3-ad0f-4685-b9e8-704e77934775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b5f2fa3-ad0f-4685-b9e8-704e77934775",
        "outputId": "f6f20ece-0d33-4fdf-d4fd-6e1f7b97650d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: execute_sql\\nTool Description: execute_sql(sql: str) -> str\\nGiven a SQL input string, execute it.\\nTool Args: {\"type\": \"object\", \"properties\": {\"sql\": {\"title\": \"Sql\", \"type\": \"string\"}}, \"required\": [\"sql\"]}\\n\\n> Tool Name: add\\nTool Description: add(a: int, b: int) -> int\\nAdd two numbers.\\nTool Args: {\"type\": \"object\", \"properties\": {\"a\": {\"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"]}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of execute_sql, add) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n', additional_kwargs={}),\n",
              " ChatMessage(role=<MessageRole.USER: 'user'>, content='Can you find the top three rows from the table named `revenue_years`', additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "chat_formatter = ReActChatFormatter()\n",
        "output_parser = ReActOutputParser()\n",
        "input_msgs = chat_formatter.format(\n",
        "    tools,\n",
        "    [\n",
        "        ChatMessage(\n",
        "            content=\"Can you find the top three rows from the table named `revenue_years`\",\n",
        "            role=\"user\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "input_msgs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6ed9b3-aeb8-453b-825a-f711d907cb5a",
      "metadata": {
        "id": "4f6ed9b3-aeb8-453b-825a-f711d907cb5a"
      },
      "source": [
        "Next we get the output from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f4685858-92d6-4dd9-b23a-5394c97991a8",
      "metadata": {
        "id": "f4685858-92d6-4dd9-b23a-5394c97991a8"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model=\"gpt-4-1106-preview\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0f2330aa-7ac7-418f-bb22-cdd1eb081626",
      "metadata": {
        "id": "0f2330aa-7ac7-418f-bb22-cdd1eb081626"
      },
      "outputs": [],
      "source": [
        "response = llm.chat(input_msgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785cf896-1dff-49ba-97ba-a7ed6bae0c1d",
      "metadata": {
        "id": "785cf896-1dff-49ba-97ba-a7ed6bae0c1d"
      },
      "source": [
        "Finally we use our ReActOutputParser to parse the content into a structured output, and analyze the action inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1ab1439f-ac69-4e36-97d6-befb3bd6b40e",
      "metadata": {
        "id": "1ab1439f-ac69-4e36-97d6-befb3bd6b40e"
      },
      "outputs": [],
      "source": [
        "reasoning_step = output_parser.parse(response.message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bf3ee527-5f34-41e7-8309-2bc4db785f1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf3ee527-5f34-41e7-8309-2bc4db785f1c",
        "outputId": "c480978b-79df-4630-f063-9fd95c609c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sql': 'SELECT * FROM revenue_years ORDER BY revenue DESC LIMIT 3'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "reasoning_step.action_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSOBy52nUKLj"
      },
      "id": "LSOBy52nUKLj",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZXpXyXpdU6zN"
      },
      "id": "ZXpXyXpdU6zN"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ppIX_SpUn7p"
      },
      "id": "5ppIX_SpUn7p",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
      },
      "source": [
        "# Multi-Step Query Decomposition\n",
        "\n",
        "We have a multi-step query engine that's able to decompose a complex query into sequential subquestions. This\n",
        "guide walks you through how to set it up!"
      ],
      "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5885d63"
      },
      "source": [
        "#### Download Data"
      ],
      "id": "c5885d63"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ee4ace96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21d41b2-3bad-4149-bb9e-32505cb835a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 08:45:24--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: data/paul_graham/paul_graham_essay.txt\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-27 08:45:24 (4.42 MB/s) - data/paul_graham/paul_graham_essay.txt saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "id": "ee4ace96"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
      },
      "source": [
        "#### Load documents, build the VectorStoreIndex"
      ],
      "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "65ebfca2"
      },
      "outputs": [],
      "source": [],
      "id": "65ebfca2"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from IPython.display import Markdown, display"
      ],
      "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "c48da73f-aadb-480c-8db1-99c915b7cc1c"
      },
      "outputs": [],
      "source": [
        "# LLM (gpt-3.5)\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n"
      ],
      "id": "c48da73f-aadb-480c-8db1-99c915b7cc1c"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "03d1691e-544b-454f-825b-5ee12f7faa8a"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ],
      "id": "03d1691e-544b-454f-825b-5ee12f7faa8a"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
      },
      "source": [
        "#### Query Index"
      ],
      "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "95d989ba-0c1d-43b6-a1d3-0ea7135f43a6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "\n",
        "step_decompose_transform = StepDecomposeQueryTransform(llm=gpt35, verbose=True)\n",
        "\n",
        "# gpt-3\n",
        "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
        "    llm=gpt35, verbose=True\n",
        ")"
      ],
      "id": "95d989ba-0c1d-43b6-a1d3-0ea7135f43a6"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2a124db0-e2d7-4566-bcec-1d41cf669ff4"
      },
      "outputs": [],
      "source": [
        "index_summary = \"Used to answer questions about the author\""
      ],
      "id": "2a124db0-e2d7-4566-bcec-1d41cf669ff4"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
        "outputId": "1735be70-f77c-4dd3-b2ac-e046094af91c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Who was in the first batch of the accelerator program the author started?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Who started the accelerator program?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Who was in the first batch of the accelerator program the author started?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Who was in the first batch of Paul Graham's accelerator program?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Who was in the first batch of the accelerator program the author started?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Who were some of the individuals in the first batch of the accelerator program started by the author?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "from llama_index.core.query_engine import MultiStepQueryEngine\n",
        "\n",
        "query_engine = index.as_query_engine(llm=gpt35)\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine,\n",
        "    query_transform=step_decompose_transform,\n",
        "    index_summary=index_summary,\n",
        ")\n",
        "response_gpt_1 = query_engine.query(\n",
        "    \"Who was in the first batch of the accelerator program the author\"\n",
        "    \" started?\",\n",
        ")"
      ],
      "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
        "outputId": "e5b0293e-31cb-4ff8-eb2f-19b8b7dd7f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>reddit, Justin Kan and Emmett Shear, Aaron Swartz, and Sam Altman were in the first batch of the accelerator program the author started.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(f\"<b>{response_gpt_1}</b>\"))"
      ],
      "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1c9670bd-729d-478b-a77c-c6e13c282456",
        "outputId": "3a204cec-3c2f-4905-dff5-1f8e943a7866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Who started the accelerator program?', 'Paul Graham started the accelerator program.'), (\"Who was in the first batch of Paul Graham's accelerator program?\", \"The first batch of Paul Graham's accelerator program included reddit, Justin Kan and Emmett Shear, Aaron Swartz, and Sam Altman.\"), ('Who were some of the individuals in the first batch of the accelerator program started by the author?', 'The first batch of the accelerator program started by the author included reddit, Justin Kan and Emmett Shear, Aaron Swartz, and Sam Altman.')]\n"
          ]
        }
      ],
      "source": [
        "sub_qa = response_gpt_1.metadata[\"sub_qa\"]\n",
        "tuples = [(t[0], t[1].response) for t in sub_qa]\n",
        "print(tuples)"
      ],
      "id": "1c9670bd-729d-478b-a77c-c6e13c282456"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ec88df57",
        "outputId": "a7e4c50f-813c-4bd8-ab6e-d54b0b3785fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where was the author's friend Robert's apartment located when he founded his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where was the author's friend Robert's apartment located when he founded his first company, Viaweb?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_gpt_1 = query_engine.query(\n",
        "    \"In which city did the author found his first company, Viaweb?\",\n",
        ")"
      ],
      "id": "ec88df57"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "653508f1-b2b0-479a-85b3-113cda507231",
        "outputId": "f6f1fcdf-dfd1-4796-f60e-c6a5b8159fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author founded his first company, Viaweb, in Cambridge.\n"
          ]
        }
      ],
      "source": [
        "print(response_gpt_1)"
      ],
      "id": "653508f1-b2b0-479a-85b3-113cda507231"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9fa93cdb-7007-4664-853a-5c81c6c17560",
        "outputId": "c8ed47e9-65b6-433b-e6c3-168cdf6f3cd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where was the author working when he founded his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: In which city did the author found his first company, Viaweb?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Where was the author working when he founded his first company, Viaweb?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine(llm=gpt35)\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine,\n",
        "    query_transform=step_decompose_transform_gpt3,\n",
        "    index_summary=index_summary,\n",
        ")\n",
        "\n",
        "response_gpt3 = query_engine.query(\n",
        "    \"In which city did the author found his first company, Viaweb?\",\n",
        ")"
      ],
      "id": "9fa93cdb-7007-4664-853a-5c81c6c17560"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "05899fcf-7a04-4d21-9e6d-04983755d175",
        "outputId": "22ea7619-382b-4ddb-c1d3-7e1aa19d6e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author founded his first company, Viaweb, in Cambridge.\n"
          ]
        }
      ],
      "source": [
        "print(response_gpt3)"
      ],
      "id": "05899fcf-7a04-4d21-9e6d-04983755d175"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbPMcQy7tKR5"
      },
      "id": "sbPMcQy7tKR5",
      "execution_count": 48,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}