{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo6WzGmty/pd5umYbbMivB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reachrkr/llm-genai/blob/main/LCEL_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sli5EgfzEFDt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    openai\\\n",
        "    docarray \\\n",
        "   tiktoken \\\n",
        "   faiss-cpu"
      ],
      "metadata": {
        "id": "SKmaX29vENhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<your-open-ap-key>\""
      ],
      "metadata": {
        "id": "d-ILEILYf7mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me small report about {topic}\"\n",
        ")\n",
        "model = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    #openai_api_key=openai_api_key\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "MgYODBDxETHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-xecWEdGX_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable"
      ],
      "metadata": {
        "id": "pKK0x_HK_KhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax follows standard Linux piping, introduced in Python. The | operator takes output from the left and feeds it into the function on the right.\n",
        "\n",
        "\n",
        "When the Python interpreter encounters the | operator connecting two objects, such as obj1 | obj2, it invokes the __or__ method of object obj2 by passing object obj1\n",
        " as an argument. This implies that the following patterns are interchangeable:\n"
      ],
      "metadata": {
        "id": "xAIcHrLmjXTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Keeping this in mind, we can create a Runnable class that takes a function, transforming it into a chainable function using the pipe operator |. This forms the essence of LCEL."
      ],
      "metadata": {
        "id": "5L_JuyV5B0B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NTpfBcDEB0Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            return other(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)\n",
        "\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "def add_two(x):\n",
        "    return x + 2\n",
        "\n",
        "\n",
        "\n",
        "# run them using the object approach\n",
        "chain_or = Runnable(add_one).__or__(Runnable(add_two))\n",
        "print(chain_or(3))\n",
        "# run  using a|b\n",
        "chain_pipe = Runnable(add_one)| (Runnable(add_two))\n",
        "print(chain_pipe(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWJThrebCxrD",
        "outputId": "3bc780ee-3d82-4d50-aaab-852064d05b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "OR1bJeDgEm5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  LCEL prompt\n",
        "Combine a prompt and a language model to create a chain. Take user input, add it to a prompt, pass it to a model, and get the raw model output. Example:"
      ],
      "metadata": {
        "id": "0HwbzK1vhYYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8zhTcJI2Glue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=model,\n",
        "    output_parser=output_parser\n",
        ")\n",
        "\n",
        "# and run\n",
        "out = chain.run(topic=\"Large Multimodal Model\")\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WTQsCezW3Rw",
        "outputId": "ff6548c9-87f6-4570-e900-b4b6f107a75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Multimodal Model (LMM) is a state-of-the-art artificial intelligence model developed by OpenAI. It is designed to understand and generate human-like text based on a given prompt, while also being capable of processing and generating images. LMM combines the power of language models with computer vision capabilities, making it a versatile tool for various applications.\n",
            "\n",
            "One of the key features of LMM is its ability to understand and generate text in a multimodal context. It can process both textual and visual information, allowing it to generate detailed and coherent responses that incorporate both words and images. This multimodal approach enables LMM to provide more comprehensive and contextually relevant outputs.\n",
            "\n",
            "LMM is trained on a massive dataset that includes a wide range of text and image data from the internet. This extensive training allows the model to learn patterns, understand context, and generate high-quality outputs. The training process involves optimizing the model's parameters to minimize the difference between its generated outputs and the desired outputs.\n",
            "\n",
            "The applications of LMM are diverse and wide-ranging. It can be used for tasks such as generating image captions, answering questions based on images, creating visual stories, and even assisting in content creation for various media platforms. LMM's ability to process and generate multimodal outputs makes it a valuable tool for industries like advertising, entertainment, and e-commerce.\n",
            "\n",
            "However, it is important to note that LMM, like any other AI model, has its limitations. It may sometimes generate outputs that are biased, inaccurate, or inappropriate. OpenAI has implemented safety measures to mitigate these risks, but it is an ongoing challenge to ensure responsible and ethical use of such models.\n",
            "\n",
            "In conclusion, Large Multimodal Model (LMM) is an advanced AI model developed by OpenAI that combines language processing with computer vision capabilities. It can understand and generate text and images in a multimodal context, making it a versatile tool for various applications. While LMM has immense potential, it is crucial to use it responsibly and address any biases or ethical concerns that may arise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LCEL\n",
        "\n",
        "lcel_chain = prompt | model | output_parser\n",
        "\n",
        "# and run\n",
        "out = lcel_chain.invoke({\"topic\": \"Large Multimodal Model\"})\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVCpe0v-Yri_",
        "outputId": "8b91074f-f373-4399-b4ed-0abda0a17164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Multimodal Model (LMM) is a state-of-the-art artificial intelligence model developed by OpenAI. It is designed to understand and generate human-like text based on a given prompt, while also being capable of processing and generating images. LMM combines the power of language models with computer vision capabilities, making it a versatile tool for various applications.\n",
            "\n",
            "One of the key features of LMM is its ability to understand and generate text in a multimodal context. It can process both textual and visual information, allowing it to generate detailed and coherent responses that incorporate both words and images. This multimodal approach enables LMM to provide more comprehensive and contextually relevant outputs.\n",
            "\n",
            "LMM is trained on a massive dataset that includes a wide range of text and image data from the internet. This extensive training allows the model to learn patterns, understand context, and generate high-quality outputs. The training process involves optimizing the model's parameters to minimize the difference between its generated outputs and the desired outputs.\n",
            "\n",
            "The applications of LMM are diverse and wide-ranging. It can be used for tasks such as generating image captions, answering questions based on images, creating visual stories, and even assisting in content creation for various media platforms. LMM's ability to process and generate multimodal outputs makes it a valuable tool for industries like advertising, entertainment, and e-commerce.\n",
            "\n",
            "However, it is important to note that LMM, like any other AI model, has its limitations. It may sometimes generate outputs that are biased, incorrect, or nonsensical. Additionally, LMM's performance heavily relies on the quality and diversity of the training data it has been exposed to. Therefore, continuous improvement and refinement of the model are necessary to address these limitations and enhance its capabilities.\n",
            "\n",
            "In conclusion, Large Multimodal Model is an advanced AI model developed by OpenAI that combines language processing with computer vision capabilities. It can generate text and images in a multimodal context, making it a versatile tool for various applications. While it has its limitations, LMM represents a significant advancement in the field of AI and holds great potential for enhancing human-computer interactions and content generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To regulate text generation in your chain, you have the option to incorporate stop sequences. In this configuration, the process of generating text concludes when a newline character is detected"
      ],
      "metadata": {
        "id": "SZaK0zZPGnom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cb83rkfHLSW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model.bind(stop=[\"\\n\"])\n",
        "result = chain.invoke({\"topic\": \"Large Multimodal Model\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jvzkedGGulg",
        "outputId": "dd425c4b-9ea4-4ef4-c048-5156ae6115b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Large Multimodal Model (LMM) is a state-of-the-art artificial intelligence model developed by OpenAI. It is designed to understand and generate human-like text based on a given prompt, while also being capable of processing and generating images. LMM combines the power of language models with computer vision capabilities, making it a versatile tool for various applications.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL facilitates the attachment of function call information to your chain, enhancing the functionality and providing valuable context during text generatio.This example attaches function call information to generate a summary."
      ],
      "metadata": {
        "id": "M4JHBo_HOlR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"summary\",\n",
        "        \"description\": \"A summary\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"setup\": {\"type\": \"string\", \"description\": \"LMM summary\"},\n",
        "                \"punchline\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Summary\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"setup\", \"punchline\"],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "chain = prompt | model.bind(function_call={\"name\": \"summary\"}, functions=functions)   | JsonOutputFunctionsParser()\n",
        "result = chain.invoke({\"topic\": \"Large Multimodal Model\"}, config={})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8F7UhKqOllf",
        "outputId": "ef235c64-983c-41f3-9eb1-df9551dfcbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'setup': 'Large Multimodal Model (LMM) is a state-of-the-art model that combines multiple modalities, such as text, image, and audio, to perform various tasks.', 'punchline': 'LMM has achieved impressive results in tasks like image captioning, visual question answering, and speech recognition, surpassing previous models in terms of performance and versatility.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL enables the creation of retrieval-augmented generation chains, merging retrieval and language generation steps for a comprehensive and sophisticated approach to content creation"
      ],
      "metadata": {
        "id": "1DD83SexfZGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create a vector store and retriever\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\" ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings\",\"Artificially induced intelligence\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Define templates for prompts\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# Create a retrieval-augmented generation chain\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"what is Artificial intelligence?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L-Qo5WwfZq3",
        "outputId": "456cc735-8fc6-49dd-d8f0-dc520e8a5316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence refers to the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTkGdOsAfmGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Chains\n"
      ],
      "metadata": {
        "id": "YfKnTBbnufTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The integration of Runnables allows for the concatenation of multiple chains, enabling a seamless connection between distinct processes for enhanced and cohesive text generation.\n",
        "\n",
        "In this instance, a branching and merging chain is applied to construct a rationale, analyze its merits and drawbacks, and then generate a conclusive response.\n"
      ],
      "metadata": {
        "id": "S5X8a7R9uoxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"is this {city} caputal of this country?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain2.invoke({\"city\": \"Rome\", \"language\": \"spanish\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQEwjv5upPy",
        "outputId": "08209b07-b9e9-42a2-9710-505bf7291c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lo siento, pero necesito más contexto para responder tu pregunta con precisión. ¿Podrías especificar a qué país te refieres?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Branching and Merging**\n",
        "\n",
        "LCEL facilitates the splitting and merging of chains through RunnableMaps. Here's an example illustrating branching and merging:"
      ],
      "metadata": {
        "id": "G11sHxSvQ25G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8rxd2C9iQp7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the pros or positive aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the cons or negative aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"ai\", \"{original_response}\"),\n",
        "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "            (\"system\", \"Generate a final response given the critique\"),\n",
        "        ]\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "result = chain.invoke({\"input\": \"Agile\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g32u9CcbuvGv",
        "outputId": "f15c6900-b42c-4aae-9fbc-26f4dbcf7a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While Agile methodology has numerous benefits, it is important to consider the potential drawbacks and assess whether they align with the specific project requirements and organizational culture. The lack of predictability and potential for increased complexity can be challenging for organizations with strict deadlines or limited resources. Additionally, the high level of dependency on customer involvement can slow down the development process if the customer is not readily available or lacks a clear understanding of their requirements. Scaling Agile methodologies across large organizations or complex projects can also be a significant undertaking. The prioritization of working software over comprehensive documentation can make it difficult to track changes or troubleshoot issues. Additionally, the flexible nature of Agile can sometimes lead to scope creep if additional features or requirements are added without proper evaluation. Effective team collaboration and communication are crucial for success, and any gaps or conflicts within the team can hinder progress. Overall, while Agile methodology offers numerous benefits, organizations should carefully consider its potential drawbacks and determine if it is the best fit for their specific needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch, Stream & Async"
      ],
      "metadata": {
        "id": "Yrqhow7i2h0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch processing** is enhanced through LangChain's Expression Language, streamlining LLM queries by executing multiple tasks concurrently. LangChain's batch functionality optimizes inputs by employing parallel LLM calls, ensuring efficient and improved performance in interactions with the LLM model."
      ],
      "metadata": {
        "id": "FyOi4u4z2uHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate.from_template(\"Given the items: {items}, What games can I play\")\n",
        "chain = prompt | model |  StrOutputParser()\n",
        "response = chain.batch([{\"items\": \"bat, ball, gloves\"},{\"items\":\"stick, ball, gloves, pads\"}])\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqFy1GHW2kQL",
        "outputId": "10e257ba-da60-434d-c805-91c41dbf781b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['With the items bat, ball, and gloves, you can play various games, including:\\n\\n1. Baseball: Use the bat to hit the ball and play a game of baseball, either individually or with a team.\\n2. Softball: Similar to baseball, but with a larger ball and a different set of rules.\\n3. Cricket: Use the bat and ball to play the sport of cricket, either in a formal setting or casually with friends.\\n4. Catch: Use the gloves to play a simple game of catch, throwing the ball back and forth with a partner.\\n5. T-Ball: Use the bat and ball to play a simplified version of baseball, where the ball is placed on a stationary tee for easier hitting.\\n6. Wiffle Ball: Play a modified version of baseball using a lightweight plastic ball and a plastic bat, suitable for smaller spaces.\\n7. Kickball: Although primarily played with a larger ball, you can use the bat to kick the ball instead, adding a twist to the traditional game.\\n8. Handball: Use the gloves to play a game of handball, hitting the ball against a wall and trying to outmaneuver your opponent.\\n9. Glove Toss: Use the gloves to play a game where you toss the ball back and forth, trying to catch it in the gloves.\\n10. Home Run Derby: Set up a batting area and see how many home runs you can hit with the bat and ball.', 'With the given items, you can play various games such as:\\n1. Baseball: Use the stick as a bat, the ball for pitching, and the gloves for catching.\\n2. Hockey: Utilize the stick as a hockey stick, the ball as a puck, and the pads for protection.\\n3. Cricket: Use the stick as a cricket bat, the ball for bowling, and the gloves for fielding.\\n4. Lacrosse: Utilize the stick as a lacrosse stick, the ball for playing, and the gloves for protection.\\n5. Golf: Use the stick as a golf club, the ball for hitting, and the gloves for grip and control.\\n6. Tennis: Utilize the stick as a tennis racket, the ball for playing, and the gloves for better grip.\\n7. Field Hockey: Use the stick as a field hockey stick, the ball for playing, and the pads for protection.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stream functionality** in LangChain facilitates immediate data flow, making it well-suited for dynamic chatbots and live-stream applications. ChefBot exemplifies this capability by seamlessly streaming information, eliminating any wait time and showcasing the power of LangChain in dynamic contexts."
      ],
      "metadata": {
        "id": "8vjZ0Jat7WST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model\n",
        "for s in chain.stream({\"items\": \"ball ,jersey, shoes\"}):print(s.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmYJQrD77Yvk",
        "outputId": "7519aa77-5728-446f-8b4f-ee94cc6c750b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several games you can play with the items ball, jersey, and shoes. Here are a few options:\n",
            "\n",
            "1. Basketball: Use the ball and shoes to play a game of basketball. You can wear the jersey to represent your favorite team.\n",
            "\n",
            "2. Soccer or Football: Use the ball and shoes to play a game of soccer or football. The jersey can be worn to represent your team or just for fun.\n",
            "\n",
            "3. Dodgeball: Use the ball and shoes to play a game of dodgeball. The jersey can help differentiate teams or just add to the fun.\n",
            "\n",
            "4. Baseball or Softball: Use the ball and shoes to play a game of baseball or softball. The jersey can be worn to represent your team or just for a sporty look.\n",
            "\n",
            "5. Kickball: Use the ball and shoes to play a game of kickball. The jersey can be worn for team identification or just for a sporty vibe.\n",
            "\n",
            "6. Volleyball: Use the ball and shoes to play a game of volleyball. While you may not need the jersey for this game, you can wear it for a sporty look or to represent a team.\n",
            "\n",
            "These are just a few examples, but there are many other games you can play with these items depending on your interests and preferences."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By leveraging ainvoke and await methods, a seamless **asynchronous execution** is achieved. This empowers tasks to operate independently, substantially enhancing responsiveness and application speed within the realm of asynchronous capabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yTrmYFfP8LT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await chain.ainvoke({\"items\": \"shuttle cock, bat\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMvjOWIe8N_a",
        "outputId": "76747512-317b-4dec-c496-0c63908c0a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='With these items, you can play the game of badminton.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallelize steps"
      ],
      "metadata": {
        "id": "0axR5EyJ8kc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnableParallel, also known as RunnableMap, simplifies the simultaneous execution of multiple Runnables. It seamlessly returns the output of these Runnables as a map, providing an efficient and straightforward approach to parallel processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qjjfhwcI99Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "model = ChatOpenAI()\n",
        "story_chain = ChatPromptTemplate.from_template(\"tell me a story about {topic}\") | model\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
        ")\n",
        "\n",
        "map_chain = RunnableParallel(story=story_chain, poem=poem_chain)\n",
        "\n",
        "map_chain.invoke({\"topic\": \"goofy\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL3JyTQJ8fUc",
        "outputId": "ecbc28f6-d0f1-4910-93a5-0fdf855f6da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'story': AIMessage(content=\"Once upon a time in the whimsical land of Disney, there lived a lovable and clumsy character named Goofy. Goofy was a tall, anthropomorphic dog with a heart of gold but an extraordinary knack for getting into hilarious and unpredictable situations.\\n\\nOne sunny morning, Goofy woke up with an idea buzzing in his head. He wanted to surprise his best friends, Mickey and Donald, with a special homemade picnic. With his characteristic enthusiasm, he gathered all the ingredients and set off to the beautiful countryside.\\n\\nAs Goofy strolled through the fields, he couldn't help but notice a colorful kite soaring high above him. Fascinated, he decided to give it a try. Without hesitation, he grabbed the kite and started running, hoping to feel the thrill of flying like the kite. However, his clumsiness got the best of him, and he ended up tripping over his own feet, sending the kite spiraling towards a nearby tree. Goofy's face turned beet red as he sheepishly retrieved the tangled kite from the branches.\\n\\nUndeterred by his failure, Goofy continued his journey to find the perfect picnic spot. He stumbled upon a picturesque meadow with a bubbling brook nearby. Excitedly, he began to unpack the picnic basket and laid out the delicious treats he had prepared – sandwiches, fruits, and his famous homemade pies.\\n\\nJust as Goofy was about to sit down and enjoy his feast, a group of mischievous squirrels appeared. They were drawn to the tempting aroma of Goofy's food and couldn't resist causing a ruckus. They jumped and hopped around, playfully stealing bites from the sandwiches and rolling away the apples. Goofy tried his best to shoo them away, but they were too quick for him. Instead, he found himself engaged in a comical game of chase around the meadow, trying to reclaim his food from the rascally squirrels.\\n\\nEventually, the squirrels disappeared into the trees, leaving Goofy breathless but laughing at the absurdity of the situation. He sat down amidst the scattered picnic items, still smiling, and decided to enjoy what was left. After taking a bite of his pie, he realized it was filled with something unexpected – whipped cream! It turned out Goofy had mixed up the ingredients and accidentally created a pie that was both sweet and savory. With a shrug and a chuckle, he took another bite, finding the unusual flavor surprisingly delightful.\\n\\nAs the day drew to a close, Goofy finally made his way back to Mickey and Donald with the remnants of the picnic. Seeing his friends, he couldn't help but share his misadventures, making them laugh until their stomachs hurt. They all agreed that even though Goofy's surprises didn't go as planned, the memories made were unforgettable and the laughter was priceless.\\n\\nAnd so, Goofy's day ended on a high note, proving that even with his trademark clumsiness, he always managed to see the silver lining and spread joy to those around him. From that day forward, Goofy's friends knew that whenever he was involved, there would never be a dull moment, and they cherished him for his unique and endearing personality.\"),\n",
              " 'poem': AIMessage(content='Goofy dances, laughter ensues,\\nHis spirit shines in shades of hues.')}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute custom functions."
      ],
      "metadata": {
        "id": "GYZJrIA1-0YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RunnableLambda, within LangChain, serves as an abstraction enabling the transformation of custom Python functions into pipe functions.\n",
        "\n",
        "This example bears similarity to the Runnable class introduced earlier in the article.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Nx-U72X_pbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "def add_two(x):\n",
        "    return x + 2\n",
        "\n",
        "# wrap the functions with RunnableLambda\n",
        "add_one = RunnableLambda(add_one)\n",
        "add_two = RunnableLambda(add_two)\n",
        "chain = add_one | add_two\n",
        "chain.invoke(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plgWReFI-RH0",
        "outputId": "0ad9e851-47a6-43ea-e3aa-495fe621c24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "As we conclude this examination, it's essential to recognize that the Langchain Expression Language (LCEL) spans beyond the subjects we've covered. LCEL effortlessly delves into diverse domains, encompassing Conversational Retrieval Chains, Multi-LLM Chain Fusion, Tools Integration, Memory Enhancement, SQL Querying, and Python REPL Coding, showcasing its versatility and broad functionality.\n"
      ],
      "metadata": {
        "id": "fKf-wDsDFn9H"
      }
    }
  ]
}