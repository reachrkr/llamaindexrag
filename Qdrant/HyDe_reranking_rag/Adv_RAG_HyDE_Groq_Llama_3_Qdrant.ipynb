{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6gfR4TA5i7S",
        "outputId": "088cf4e1-fc27-443a-ffc5-d8db796af89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.66.1)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.66.1)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.28.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (71.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 numpy sentence-transformers groq qdrant-client python-dotenv arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Arxiv PDF document to be used for injecting in Qdrant"
      ],
      "metadata": {
        "id": "fVa6-o_cfQCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "# Specify the arXiv ID with version number (e.g., '1707.08567v2')\n",
        "arxiv_id = \"2408.16765v1\"\n",
        "\n",
        "# Search for the specific version of the paper using the arXiv ID\n",
        "search = arxiv.Search(\n",
        "    id_list=[arxiv_id]\n",
        ")\n",
        "\n",
        "# Download the PDF of the specific version\n",
        "for result in search.results():\n",
        "    print(f\"Downloading: {result.title} (version: {arxiv_id})\")\n",
        "    result.download_pdf( filename=arxiv_id+\".pdf\")\n",
        "    print(\"Download completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5ySS1uRabSF",
        "outputId": "cdd54b3f-5962-4740-98d7-45dc93a9faac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-ed97c7cb79ed>:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: A Score-Based Density Formula, with Applications in Diffusion Generative Models (version: 2408.16765v1)\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heMEAwmkfnHg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyDe with Qdrant DB"
      ],
      "metadata": {
        "id": "BDR6xb1Hiacc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the lIbraries"
      ],
      "metadata": {
        "id": "liWdWJN5fndT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#import openai\n",
        "from groq import Groq\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72cg9sImbwWY",
        "outputId": "1db7a5db-a732-407d-bd88-f87271fd8a14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set up API Grok and Qdrant"
      ],
      "metadata": {
        "id": "eXTeHLTzfroS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up API clients\n",
        "groq_client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n",
        "qdrant_client = QdrantClient(userdata.get('QDRANT_URL'), api_key=userdata.get('QDRANT_KEY'))\n",
        "\n",
        "#qdrant_client = QdrantClient(os.getenv('QDRANT_URL'), api_key=os.getenv('QDRANT_API_KEY'))"
      ],
      "metadata": {
        "id": "9w-nA5b2Fcsj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constants"
      ],
      "metadata": {
        "id": "FLcnJSG2fw5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "kt4EoYZJDBaU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "COLLECTION_NAME = \"document_chunks\"\n",
        "VECTOR_SIZE = 384  # Dimension of the sentence-transformers model output"
      ],
      "metadata": {
        "id": "JGfgSYa7-5O1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utililty functions"
      ],
      "metadata": {
        "id": "5m0A-Wrlf0RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Define utility functions\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "UyNbQ_DGCvFg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += (chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVZ7PZwnCx3v",
        "outputId": "c409bfef-2b5b-4de8-98b9-8fc5f8579011"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(chunks):\n",
        "    return model.encode(chunks)"
      ],
      "metadata": {
        "id": "qCPwwGA9Ynrl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qdrant vector database functions"
      ],
      "metadata": {
        "id": "x6pBk1zUiDGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Qdrant functions\n",
        "def setup_qdrant_collection():\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE),\n",
        "    )\n",
        "\n",
        "def store_embeddings_in_qdrant(chunks, embeddings):\n",
        "    qdrant_client.upsert(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        points=[\n",
        "            models.PointStruct(\n",
        "                id=idx,\n",
        "                vector=embedding.tolist(),\n",
        "                payload={\"text\": chunk}\n",
        "            )\n",
        "            for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def retrieve_relevant_chunks_qdrant(query_embedding, top_k=3):\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=top_k\n",
        "    )\n",
        "    return [hit.payload['text'] for hit in search_result]\n"
      ],
      "metadata": {
        "id": "leH4KpyYC2YL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Model (LLM) functions"
      ],
      "metadata": {
        "id": "qnTRUOMoiHGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define LLM functions\n",
        "def llm_call(prompt, provider='openai', model_name='gpt-3.5-turbo'):\n",
        "    if provider == 'groq':\n",
        "        chat_completion = groq_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1000,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Choose 'openai' or 'groq'.\")"
      ],
      "metadata": {
        "id": "9umRkqzkY2_s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hyde_document(query, provider='groq', model_name='llama3-8b-8192'):\n",
        "    prompt = f\"Given the question '{query}', generate a short, relevant passage that might answer this question:\"\n",
        "    return llm_call(prompt, provider, model_name)"
      ],
      "metadata": {
        "id": "psyNhu8eFpRv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_with_hyde(query, provider='groq', model_name='llama3-8b-8192'):\n",
        "    hyde_doc = generate_hyde_document(query, provider, model_name)\n",
        "    print(hyde_doc)\n",
        "    hyde_embedding = model.encode([hyde_doc])[0]\n",
        "    #print(hyde_embedding)\n",
        "    relevant_chunks = retrieve_relevant_chunks_qdrant(hyde_embedding)\n",
        "    print(relevant_chunks)\n",
        "    context = \" \".join(relevant_chunks)\n",
        "\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a concise and accurate answer based on the given context. If the context doesn't contain enough information to answer the question fully, please state that and provide the best possible answer with the available information.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return llm_call(prompt, provider, model_name)"
      ],
      "metadata": {
        "id": "yqLMYw_WFw8G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pc02z_sQiL3t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution"
      ],
      "metadata": {
        "id": "afB44pw_iMO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Main execution\n",
        "# Replace 'your_document.pdf' with the path to your actual PDF file\n",
        "pdf_path = '2408.16765v1.pdf'\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "chunk_embeddings = generate_embeddings(text_chunks)"
      ],
      "metadata": {
        "id": "fEPJVbKyF5pk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and populate Qdrant collection\n",
        "setup_qdrant_collection()\n",
        "store_embeddings_in_qdrant(text_chunks, chunk_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lD_YM4If5N-",
        "outputId": "9f1274ed-9393-45e2-9f65-a0de4171bc4c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1520bb626edb>:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant_client.recreate_collection(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying the system"
      ],
      "metadata": {
        "id": "W5lVAekfiQ_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Query the system\n",
        "query = \"What are the main topics discussed in the document?\"\n",
        "\n",
        "# Using Groq with Llama 3\n",
        "answer_groq_llama3 = rag_with_hyde(query, provider='groq', model_name='llama3-8b-8192')\n",
        "print(\"Groq with Llama 3 Answer:\", answer_groq_llama3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kho5yYfGE9Q",
        "outputId": "e05b70b2-78ec-4bf7-bdc2-54f799cf823d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide the document, and I'll generate a short passage that summarizes the main topics discussed.\n",
            "['oss in autoregressive model s. . . . . . . . . . . . . . . . . . . . . 10\\n5 Proof of Theorem 1 10\\n6 Discussion 13\\n∗The authors contributed equally.\\n†Department of Statistics, The Chinese University of Hong Ko ng, Hong Kong; Email: genli@cuhk.edu.hk .\\n‡Department of Statistics, University of Wisconsin-Madiso n, Madison, WI 53706, USA; Email: yuling.yan@wisc.edu .\\n1A Proof of Proposition 1 14\\nB Proof of Proposition 2 16\\nC More discussions on the density formulas 19\\nD Technical details in Section 4 19\\nD.1 Technical details in Section 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nD.2 Technical details in Section 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n1 Introduction\\nScore-based generative models (SGMs) represent a groundbr eaking advancement in the realm of generative\\nmodels, signiﬁcantly impacting machine learning and artiﬁ cial intelligence by their ability to synthesize\\nhigh-ﬁdelity data instances, including images, aud', 'th deep\\nlanguage understanding. Advances in Neural Information Processing Systems , 35:36479–36494.\\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ga nguli, S. (2015). Deep unsupervised learning\\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning , pages 2256–\\n2265.\\nSong, J., Meng, C., and Ermon, S. (2021a). Denoising diﬀusio n implicit models. In International Conference\\non Learning Representations .\\nSong, Y. and Ermon, S. (2019). Generative modeling by estima ting gradients of the data distribution.\\nAdvances in neural information processing systems , 32.\\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermo n, S., and Poole, B. (2021b). Score-based\\ngenerative modeling through stochastic diﬀerential equat ions.International Conference on Learning Rep-\\nresentations .\\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation ,\\n23(7):1661–1674.\\nXia, M., Shen, Y., Yang, C., Yi, R., Wang, W., and ', 'und Lvb. It is known that the global minimizer for\\nEx∼q0[L(x)] =T/summationdisplay\\nt=11−αt+1\\n2(1−αt)Ex∼q0,ε∼N(0,Id)/bracketleftBig/vextenddouble/vextenddoubleε−εt(√αtx+√\\n1−αtε)/vextenddouble/vextenddouble2\\n2/bracketrightBig\\n(4.5)\\nis exactly/hatwideεt(·)≡ε⋆\\nt(·)for each 1≤t≤T(see Appendix D.1). Although in practice the optimization is\\nbased on samples from the target distribution q0(instead of the population level expectation over q0) and\\nmay not ﬁnd the exact global minimizer, we consider the ideal scenario where the learned epsilon predictors\\n/hatwideεtequalε⋆\\ntto facilitate discussion. When εt=ε⋆\\ntfor each t, according to ( 4.1), we have\\nL(x)≈−logq0(x)+C⋆\\n0. (4.6)\\nTaking ( 4.4) and ( 4.6) together gives\\n0≤L(/hatwideε1,...,/hatwideεT)≤Lvb(/hatwideε1,...,/hatwideεT)≈−Ex∼q0[logq(x)]+C⋆\\n0−C⋆\\n0−H(q0) = 0, (4.7)\\nnamely the minimizer for Lvbapproximately minimizes L, and the optimal value is asymptotically zero when\\nthe number of steps Tbecomes large. This suggests that by minimizing the va']\n",
            "Groq with Llama 3 Answer: Based on the provided context, the main topics discussed in the document appear to be:\n",
            "\n",
            "1. Score-based generative models (SGMs)\n",
            "2. Denoising diffusion implicit models\n",
            "3. Generative modeling by estimating gradients of the data distribution\n",
            "4. Score-based generative modeling through stochastic differential equations\n",
            "5. Connection between score matching and denoising autoencoders\n",
            "\n",
            "These topics are mentioned in the introduction, and further discussion is provided in the subsequent sections, including proof of propositions and technical details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional interactive querying"
      ],
      "metadata": {
        "id": "zJ7GTsXAiUJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  (Optional) Interactive querying\n",
        "def interactive_query():\n",
        "    while True:\n",
        "        query = input(\"Enter your question (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        answer = rag_with_hyde(query)\n",
        "        print(\"Answer:\", answer)\n",
        "        print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "GiNQYEWgYKFm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interactive_query()"
      ],
      "metadata": {
        "id": "_82uAMOdYRS7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReRanking with Hyde"
      ],
      "metadata": {
        "id": "pLtVWntgioO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n"
      ],
      "metadata": {
        "id": "aow-03kPiw9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "COLLECTION_NAME = \"document_chunks_reranked\"\n",
        "VECTOR_SIZE = 384  # Dimension of the sentence-transformers model output"
      ],
      "metadata": {
        "id": "-FKePO8KEy1m"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QiBoRftLpfEX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder"
      ],
      "metadata": {
        "id": "18OtBO46eVyq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiliase the models"
      ],
      "metadata": {
        "id": "SsKQ6J1skCSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
      ],
      "metadata": {
        "id": "7sh56dANgB_0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "B9kFQN4WkHFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += (chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings(chunks):\n",
        "    return bi_encoder.encode(chunks)\n",
        "\n"
      ],
      "metadata": {
        "id": "aGRquMqUh_td"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qdrant Functions"
      ],
      "metadata": {
        "id": "HzoqME1wkRkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Qdrant functions\n",
        "def setup_qdrant_collection():\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE),\n",
        "    )\n",
        "\n",
        "def store_embeddings_in_qdrant(chunks, embeddings):\n",
        "    qdrant_client.upsert(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        points=[\n",
        "            models.PointStruct(\n",
        "                id=idx,\n",
        "                vector=embedding.tolist(),\n",
        "                payload={\"text\": chunk}\n",
        "            )\n",
        "            for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def retrieve_relevant_chunks_qdrant(query_embedding, top_k=10):  # Increased top_k for reranking\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=top_k\n",
        "    )\n",
        "    return [(hit.payload['text'], hit.score) for hit in search_result]"
      ],
      "metadata": {
        "id": "1NtGuS0ZiDHo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution:\n"
      ],
      "metadata": {
        "id": "Cik_1mHAkV5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Main execution\n",
        "# Replace 'your_document.pdf' with the path to your actual PDF file\n",
        "pdf_path = '2408.16765v1.pdf'\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "chunk_embeddings = generate_embeddings(text_chunks)"
      ],
      "metadata": {
        "id": "Vub_wbb1lyRS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and populate Qdrant collection\n",
        "setup_qdrant_collection()\n",
        "store_embeddings_in_qdrant(text_chunks, chunk_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRN2F6rRqbuB",
        "outputId": "8f86618a-7200-43bd-927c-0cf47e023e7e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-f6c35d002c17>:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant_client.recreate_collection(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reranking & RAG Function using HyDe\n"
      ],
      "metadata": {
        "id": "85N33Jcxkb2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define reranking function\n",
        "def rerank_with_hyde(query, retrieved_chunks, top_k=3):\n",
        "    hyde_doc = generate_hyde_document(query)\n",
        "\n",
        "    # Combine query and hyde_doc\n",
        "    enhanced_query = f\"{query} {hyde_doc}\"\n",
        "\n",
        "    # Prepare input pairs for cross-encoder\n",
        "    pair_inputs = [(enhanced_query, chunk) for chunk, _ in retrieved_chunks]\n",
        "\n",
        "    # Get relevance scores\n",
        "    relevance_scores = cross_encoder.predict(pair_inputs)\n",
        "\n",
        "    # Sort chunks by relevance score\n",
        "    reranked_chunks = sorted(zip(retrieved_chunks, relevance_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return top_k reranked chunks\n",
        "    return [chunk for (chunk, _), _ in reranked_chunks[:top_k]]\n",
        "\n",
        "# Updated RAG function with reranking\n",
        "def rag_with_hyde_and_reranking(query, provider='groq', model_name='llama3-8b-8192'):\n",
        "    # Generate query embedding\n",
        "    query_embedding = bi_encoder.encode([query])[0]\n",
        "\n",
        "    # Retrieve initial set of relevant chunks\n",
        "    retrieved_chunks = retrieve_relevant_chunks_qdrant(query_embedding)\n",
        "\n",
        "    # Rerank chunks using HyDE\n",
        "    reranked_chunks = rerank_with_hyde(query, retrieved_chunks)\n",
        "\n",
        "    # Combine reranked chunks into context\n",
        "    context = \" \".join(reranked_chunks)\n",
        "\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a concise and accurate answer based on the given context. If the context doesn't contain enough information to answer the question fully, please state that and provide the best possible answer with the available information.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return llm_call(prompt, provider, model_name)"
      ],
      "metadata": {
        "id": "ZXFUgpz__8Qo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dBjsHNpKlBMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v-l1uC6Yk3iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying the System:"
      ],
      "metadata": {
        "id": "jCzQ7TrYk3tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Groq with Llama 3 and reranking\n",
        "answer_groq_llama3 = rag_with_hyde_and_reranking(query, provider='groq', model_name='llama3-8b-8192')\n",
        "print(\"Groq with Llama 3 Answer (with reranking):\", answer_groq_llama3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDQjYib9_9P0",
        "outputId": "83a1ec65-3ff4-447f-fddd-046e4d0ec1e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq with Llama 3 Answer (with reranking): The main topics discussed in the document are:\n",
            "\n",
            "1. Theoretical understanding of diffusion generative models, specifically the DDPM framework.\n",
            "2. The optimization target of DDPM, which is derived from a variational lower bound on the log-likelihood (ELBO), and the lack of theoretical understanding why optimizing a lower bound is a valid approach.\n",
            "3. The use of ELBO as a proxy for the negative log-likelihood of the data distribution and its application in other generative or learning frameworks.\n",
            "\n",
            "The document also appears to discuss mathematical derivations and equations related to diffusion generative models, specifically the terms H1(x), H2(x), and H3(x), but the main topics are centered around the theoretical understanding and applications of DDPM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHvfN96Rk_TC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Querying (Optional)"
      ],
      "metadata": {
        "id": "Q-ZcN_PVlC-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional) Interactive querying\n",
        "def interactive_query_rerank_hyde():\n",
        "    while True:\n",
        "        query = input(\"Enter your question (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        answer = rag_with_hyde_and_reranking(query)\n",
        "        print(\"Answer:\", answer)\n",
        "        print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "NYq6m1qfAICq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interactive_query_rerank_hyde()"
      ],
      "metadata": {
        "id": "mwG_ZUasORAM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xN7Bgy11OODV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKnkm7y9rEt0"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}